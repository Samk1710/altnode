POST http://localhost:3000/api/runTests
{
    "model": "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
    "inputJson": "{\"question\": \"What is your favorite treasure?\"}",
    "script": "import json\nimport requests\n\ndef generate(LLM: str, messages: list[dict[str, str]], params: dict=None) -> str:\n    \"\"\"\n    Generate text outputs using the pipeline\n    \"\"\"\n    body = {\n        \"model\": LLM,\n        \"messages\": messages\n    }\n    response = requests.post(\n        url=\"https://dev.beyondnetwork.xyz/test\",\n        headers={\n            \"x-api-key\": \"test\",\n            \"Content-Type\": \"application/json\",\n        },\n        data=json.dumps(body),\n    )\n    if response.status_code != 200:\n        return json.dumps({\n            \"error\": \"Beyond API error\",\n            \"reason\": response.reason\n        })\n    response = response.json()\n    return json.dumps(response[\"choices\"][0][\"message\"][\"content\"])\n    \n\"\"\"\nUtilise the run function to define your pipeline\nThe LLM is available as a global variable called 'LLM' and can be used to generate text outputs using the 'generate' method\nThe generate method is defined as follows: generate(LLM, messages: dict, params: dict) -> str\nLLM is globally defined so you can use it directly pass it as the first argument to the generate method\nThe messages parameter is a list of dictionaries containing the input data for the pipeline\nExample: messages = [{\"role\": \"system\", \"content\": \"This is a system prompt.\"}, {\"role\": \"user\", \"content\": \"This is a user prompt.\"}]\nThe params parameter is a dictionary containing the parameters for the pipeline\nExample: params = {\"temperature\": 0.7, \"max_tokens\": 100}\n\"\"\"\n\ndef run(input_json, context=None):\n    \"\"\"\n    The 'run' function must always be defined in your code and should be the point of entry as well as output for your pipeline\n    The 'run' function should accept two parameters: input_json and context\n    input_json is a dictionary containing the input data for the pipeline\n    context is a string containing the context data for the pipeline\n    The 'run' function should return a dictionary containing the output data for the pipeline\n    The output data should be in the form of a dictionary with any number of key-value pairs that the end-user can use\n    Example: return {\"output\": \"Hello, World!\"}\n    \"\"\"\n    messages = []\n    messages.append({\"role\": \"system\", \"content\": \"You are a pirate. Speak like a pirate.\"})\n    if context:\n        messages.append({\"role\": \"user\", \"content\": \"This is the context: \" + context + \"\\nNow, answer the following question: \" + input_json[\"question\"]})\n    else:\n        messages.append({\"role\": \"user\", \"content\": \"Answer the following question: \" + input_json[\"question\"]})\n    \n    response = generate(LLM, messages, {\"temperature\": 0.7, \"max_tokens\": 100})\n\n    example_output = {\n        \"input\": input_json,\n        \"context\": context,\n        \"response\": response\n    }\n\n    return example_output\n    \n",
    "testNum": 2
}